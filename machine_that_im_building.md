# Machine that I'm building
The machine that Im building is supposed to help me find answer to at least some of the above questions.
Regard this scheme:
<br>
<br>
![main_diagram](./img/main_diagram.png)

<br>
<br>

The Mirror machine is able to recognise human movenent based on AI. 
The poses are then being stored in a memory, cached and processed by a chain of filters to emit a Midi signal. 
 - Body part sizes and rotations
 - Angles of limbs
 - Objects are being identified and tracked individually for the machine to get to know every participant individually
 - Data is then combined to emit a shared experience.

Midi protocol is a standard protocol for every musical software, or even hardware synthesiser. The output midi signal consists of 16 channels of notes and knob changes, that can be used to produce sound. 
At this point sound designer decides how to use notes and knob positions in his set up - he audio can be either set up once before session, or controlled during session, as a joined human + machine live performance.

Beyond audio machine can also control other devices, for example light and illumination, or video session for VJ.

